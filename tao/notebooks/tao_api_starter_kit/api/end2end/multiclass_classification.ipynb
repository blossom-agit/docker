{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to demonstrate Multi-Class Classification workflow\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![image](https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
    "\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "- Creating a dataset\n",
    "- Upload kitti dataset to the service\n",
    "- Running dataset convert\n",
    "- Getting a PTM from NGC\n",
    "- Model Actions\n",
    "    - Train\n",
    "    - Evaluate\n",
    "    - Prune, retrain\n",
    "    - Export\n",
    "    - Convert\n",
    "    - Inference on TAO\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Create datasets ](#head-1)\n",
    "1. [List the created datasets](#head-2)\n",
    "1. [Create model ](#head-4)\n",
    "1. [List models](#head-5)\n",
    "1. [Assign train, eval datasets](#head-6)\n",
    "1. [Assign PTM](#head-7)\n",
    "1. [Actions](#head-8)\n",
    "1. [Train](#head-9)\n",
    "1. [Evaluate](#head-10)\n",
    "1. [Optimize: Apply specs for prune](#head-12)\n",
    "1. [Optimize: Apply specs for retrain](#head-13)\n",
    "1. [Optimize: Run actions](#head-14)\n",
    "1. [Export: FP32](#head-15)\n",
    "1. [Export: INT8](#head-16)\n",
    "1. [Model convert using TAO-Converter](#head-17)\n",
    "1. [TAO inference](#head-18)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign a workdir in FIXME1\n",
    "2. Assign the ip_address and port_number in FIXME 2 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "3. Assign the ngc_api_key variable in FIXME 3\n",
    "4. Choose between default and custom dataset in FIXME 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define workspaces and other variables\n",
    "workdir = \"workdir_multiclass_classification\" # FIXME1\n",
    "host_url = \"http://<ip_address>:<port_number>\" # FIXME2 example: https://10.137.149.22:32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'\n",
    "ngc_api_key = \"<ngc_api_key>\" # FIXME3 example: zZYtczM5amdtdDcwNjk0cnA2bGU2bXQ3bnQ6NmQ4NjNhMDItMTdmZS00Y2QxLWI2ZjktNmE5M2YxZTc0OGyM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "response = requests.get(f\"{host_url}/api/v1/login/{ngc_api_key}\")\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/user/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating workdir\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `pascal VOC dataset` for the tutorial. To find more details please visit [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit). Please download the dataset present [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar) to the environment variable $DATA_DIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure, and skip running** \"**Split dataset into train and val sets**\"\n",
    "```\n",
    "DATA_DIR\n",
    "├── images_test\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_1.jpg\n",
    "│   │   ├── image_name_2.jpg\n",
    "│   │   ├── ...\n",
    "|   |   ... \n",
    "│   └── class_name_n\n",
    "│       ├── image_name_3.jpg\n",
    "│       ├── image_name_4.jpg\n",
    "│       ├── ...\n",
    "├── images_train\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_5.jpg\n",
    "│   │   ├── image_name_6.jpg\n",
    "|   |   ...\n",
    "│   └── class_name_n\n",
    "│       ├── image_name_7.jpg\n",
    "│       ├── image_name_8.jpg\n",
    "│       ├── ...\n",
    "|\n",
    "└── images_val\n",
    "    ├── class_name_1\n",
    "    │   ├── image_name_9.jpg\n",
    "    │   ├── image_name_10.jpg\n",
    "    │   ├── ...\n",
    "    |   ...\n",
    "    └── class_name_n\n",
    "        ├── image_name_11.jpg\n",
    "        ├── image_name_12.jpg\n",
    "        ├── ...\n",
    "```\n",
    "- Each class name folder should contain the images corresponding to that class\n",
    "- Same class name folders should be present across images_test, images_train and images_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"classification_data\"\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_be_used = \"default\" #FIXME4 #default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    if not os.path.exists(os.path.join(DATA_DIR,\"VOCtrainval_11-May-2012.tar\")):\n",
    "        print(\"Download VOC tar data into \", DATA_DIR)\n",
    "    else:\n",
    "        !tar -xf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    !python3 -m pip install tqdm\n",
    "    # Split dataset into train and val sets\n",
    "    from os.path import join as join_path\n",
    "    import os\n",
    "    import glob\n",
    "    import re\n",
    "    import shutil\n",
    "\n",
    "    DATA_DIR=os.environ.get('DATA_DIR')\n",
    "    source_dir = join_path(DATA_DIR, \"VOCdevkit/VOC2012\")\n",
    "    target_dir = join_path(DATA_DIR, \"formatted\")\n",
    "\n",
    "\n",
    "    suffix = '_trainval.txt'\n",
    "    classes_dir = join_path(source_dir, \"ImageSets\", \"Main\")\n",
    "    images_dir = join_path(source_dir, \"JPEGImages\")\n",
    "    classes_files = glob.glob(classes_dir+\"/*\"+suffix)\n",
    "    for file in classes_files:\n",
    "        # get the filename and make output class folder\n",
    "        classname = os.path.basename(file)\n",
    "        if classname.endswith(suffix):\n",
    "            classname = classname[:-len(suffix)]\n",
    "            target_dir_path = join_path(target_dir, classname)\n",
    "            if not os.path.exists(target_dir_path):\n",
    "                os.makedirs(target_dir_path)\n",
    "        else:\n",
    "            continue\n",
    "        print(classname)\n",
    "\n",
    "\n",
    "        with open(file) as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "\n",
    "        for line in content:\n",
    "            tokens = re.split('\\s+', line)\n",
    "            if tokens[1] == '1':\n",
    "                # copy this image into target dir_path\n",
    "                target_file_path = join_path(target_dir_path, tokens[0] + '.jpg')\n",
    "                src_file_path = join_path(images_dir, tokens[0] + '.jpg')\n",
    "                shutil.copyfile(src_file_path, target_file_path)\n",
    "\n",
    "    from random import shuffle\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    DATA_DIR=os.environ.get('DATA_DIR')\n",
    "    SOURCE_DIR=os.path.join(DATA_DIR, 'formatted')\n",
    "    TARGET_DIR=os.path.join(DATA_DIR,'split')\n",
    "    # list dir\n",
    "    print(os.walk(SOURCE_DIR))\n",
    "    dir_list = next(os.walk(SOURCE_DIR))[1]\n",
    "    # for each dir, create a new dir in split\n",
    "    for dir_i in tqdm(dir_list):\n",
    "        newdir_train = os.path.join(TARGET_DIR, 'images_train', dir_i)\n",
    "        newdir_val = os.path.join(TARGET_DIR, 'images_val', dir_i)\n",
    "        newdir_test = os.path.join(TARGET_DIR, 'images_test', dir_i)\n",
    "\n",
    "        if not os.path.exists(newdir_train):\n",
    "                os.makedirs(newdir_train)\n",
    "        if not os.path.exists(newdir_val):\n",
    "                os.makedirs(newdir_val)\n",
    "        if not os.path.exists(newdir_test):\n",
    "                os.makedirs(newdir_test)\n",
    "\n",
    "        img_list = glob.glob(os.path.join(SOURCE_DIR, dir_i, '*.jpg'))\n",
    "        # shuffle data\n",
    "        shuffle(img_list)\n",
    "\n",
    "        for j in range(int(len(img_list)*0.7)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'images_train', dir_i))\n",
    "\n",
    "        for j in range(int(len(img_list)*0.7), int(len(img_list)*0.8)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'images_val', dir_i))\n",
    "\n",
    "        for j in range(int(len(img_list)*0.8), len(img_list)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'images_test', dir_i))\n",
    "\n",
    "    print('Done splitting dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating classmap json\n",
    "classmap = {}\n",
    "for idx,folder in enumerate(sorted(os.listdir(os.path.join(DATA_DIR,\"split\",\"images_test\")))):\n",
    "    classmap[folder] = idx\n",
    "with open(os.path.join(DATA_DIR,\"split\",\"classmap.json\"), \"w\") as classmap_file:\n",
    "    json.dump(classmap, classmap_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the dataset is present\n",
    "!if [ ! -d $DATA_DIR/split/images_train ]; then echo 'train folder NOT found.'; else echo 'Found train images folder.';fi\n",
    "!if [ ! -d $DATA_DIR/split/images_val ]; then echo 'val folder NOT found.'; else echo 'Found val images folder.';fi\n",
    "!if [ ! -d $DATA_DIR/split/images_test ]; then echo 'test folder NOT found.'; else echo 'Found test images folder.';fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -C $DATA_DIR/split/ -czf classification_train.tar.gz images_train\n",
    "!tar -C $DATA_DIR/split/ -czf classification_val.tar.gz images_val\n",
    "!tar -C $DATA_DIR/split/ -czf classification_test.tar.gz images_test classmap.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_path =  \"classification_train.tar.gz\"\n",
    "eval_dataset_path = \"classification_val.tar.gz\"\n",
    "test_dataset_path =  \"classification_test.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "ds_type = \"image_classification\"\n",
    "ds_format = \"default\"\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update\n",
    "dataset_information = {\"name\":\"Train dataset\",\n",
    "                       \"description\":\"My train dataset\"}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload\n",
    "files = [(\"file\",open(train_dataset_path,\"rb\"))]\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{dataset_id}/upload\"\n",
    "\n",
    "response = requests.post(endpoint, files=files, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create eval dataset\n",
    "ds_type = \"image_classification\"\n",
    "ds_format = \"default\"\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "eval_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update\n",
    "dataset_information = {\"name\":\"Eval dataset\",\n",
    "                       \"description\":\"My eval dataset\"}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data ,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload\n",
    "files = [(\"file\",open(eval_dataset_path,\"rb\"))]\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{eval_dataset_id}/upload\"\n",
    "\n",
    "response = requests.post(endpoint, files=files, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create testing dataset for inference\n",
    "ds_type = \"image_classification\"\n",
    "ds_format = \"raw\"\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "test_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload\n",
    "files = [(\"file\",open(test_dataset_path,\"rb\"))]\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{test_dataset_id}/upload\"\n",
    "\n",
    "response = requests.post(endpoint, files=files, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the created datasets <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "# print(response.json()) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in response.json():\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_arch = \"classification\"\n",
    "encode_key = \"nvidia_tlt\"\n",
    "data = json.dumps({\"network_arch\":network_arch,\"encryption_key\":encode_key})\n",
    "\n",
    "endpoint = f\"{base_url}/model\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "model_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/model\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "# print(response.json()) ## Uncomment for verbose list output\n",
    "print(\"model id\\t\\t\\t     network architecture\")\n",
    "for rsp in response.json():\n",
    "    print(rsp[\"id\"],rsp[\"network_arch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign train, eval datasets <a class=\"anchor\" id=\"head-6\"></a>\n",
    "\n",
    "- Note: make sure the order for train_datasets is [source ID, target ID]\n",
    "- eval_dataset is kept same as target for demo purposes\n",
    "- inference_dataset is kept as target for chaining with hifigan finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_information = {\"train_datasets\":[dataset_id],\n",
    "                       \"eval_dataset\":eval_dataset_id,\n",
    "                       \"inference_dataset\":test_dataset_id}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign PTM <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "- Search for pretrained_classification:resnet18 on NGC\n",
    "- Assign it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get pretrained model for classification\n",
    "model_list = f\"{base_url}/model\"\n",
    "print(model_list)\n",
    "response = requests.get(model_list, headers=headers)\n",
    "\n",
    "response_json = response.json()\n",
    "\n",
    "# Search for ptm with given ngc path\n",
    "ptm_id = None\n",
    "for rsp in response_json:\n",
    "    if rsp[\"network_arch\"] == network_arch and \"pretrained_classification:resnet18\" in rsp[\"ngc_path\"]:\n",
    "        ptm_id = rsp[\"id\"]\n",
    "        print(\"Metadata for model with requested NGC Path\")\n",
    "        print(rsp)\n",
    "        break\n",
    "mt_class_ptm = ptm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptm_information = {\"ptm\":mt_class_ptm}\n",
    "data = json.dumps(ptm_information)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions <a class=\"anchor\" id=\"head-8\"></a>\n",
    "\n",
    "For all actions:\n",
    "1. Get default spec schema and derive the default values\n",
    "2. Modify defaults if needed\n",
    "3. Post spec dictionary to the service\n",
    "4. Run model action\n",
    "5. Monitor job using retrieve\n",
    "6. Download results using job download endpoint (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/train/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "import json\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes to the specs you want to modify\n",
    "specs[\"train_config\"][\"n_epochs\"] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/train\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "actions = [\"train\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"train\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['train']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "# Download output of multiclass_classification train (Note: will take time)\n",
    "job_id = job_map[\"train\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xvf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "model_downloaded_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for a model.tlt file\n",
    "!ls {model_downloaded_path}/weights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate <a class=\"anchor\" id=\"head-10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/evaluate/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify specs dictionary to change any config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/evaluate\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"evaluate\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"evaluate\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['evaluate']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize <a class=\"anchor\" id=\"head-11\"></a>\n",
    "\n",
    "- We optimize the trained model by pruning and retraining in the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply specs for prune <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/prune/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "# None for prune\n",
    "specs[\"pruning_threshold\"]=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/prune\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply specs for retrain <a class=\"anchor\" id=\"head-13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/retrain/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes to the specs you want to modify\n",
    "specs[\"train_config\"][\"n_epochs\"] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/retrain\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Actions <a class=\"anchor\" id=\"head-14\"></a>\n",
    "\n",
    "We use the API's job chaining feature to prune, retrain and evaluate the retrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run actions\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"prune\",\"retrain\",\"evaluate\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"prune\"] = response.json()[0]\n",
    "job_map[\"retrain\"] = response.json()[1]\n",
    "job_map[\"eval_retrain\"] = response.json()[2]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell (prune)\n",
    "job_id = job_map['prune']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell (retrain)\n",
    "job_id = job_map['retrain']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell (evaluate)\n",
    "job_id = job_map['eval_retrain']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional cancel job - for jobs that are pending/running (retrain)\n",
    "\n",
    "# job_id = job_map['retrain']\n",
    "# endpoint = f\"{base_url}/model/{model_id}/job/{job_id}/cancel\"\n",
    "\n",
    "# response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "# print(response)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional delete job - for jobs that are error/done (retrain)\n",
    "\n",
    "# job_id = job_map['retrain']\n",
    "# endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "# response = requests.delete(endpoint, headers=headers)\n",
    "\n",
    "# print(response)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export: FP32 <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/export/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes to spec dictionary if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/export\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"export\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"export_fp32\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['export_fp32']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export: INT8 <a class=\"anchor\" id=\"head-16\"></a>\n",
    "\n",
    "- Note - if another export job is in Pending state for this model experiment, modifying specs would apply to the Pending experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/export/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "specs[\"data_type\"] = \"int8\"\n",
    "specs[\"batches\"] = 10\n",
    "specs[\"batch_size\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/export\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"export\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"export_int8\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['export_int8']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "job_id = job_map[\"export_int8\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xvf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "model_downloaded_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for the generated .etlt file, tensorrt .engine file and cal.bin calibration cache file\n",
    "!ls {model_downloaded_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model convert using TAO-Converter <a class=\"anchor\" id=\"head-17\"></a>\n",
    "\n",
    "- Here, we use the INT8 exported model to convert to target platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/convert/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "specs[\"t\"] = \"int8\"\n",
    "specs[\"b\"] = 64\n",
    "specs[\"m\"] = 64\n",
    "specs[\"d\"] = \"3,224,224\"\n",
    "specs[\"i\"] = \"nchw\"\n",
    "specs[\"o\"] = \"predictions/Softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/convert\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"export_int8\"]\n",
    "actions = [\"convert\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"model_convert\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['model_convert']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAO inference <a class=\"anchor\" id=\"head-18\"></a>\n",
    "\n",
    "- Run inference on a set of images using the .tlt model created at train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/inference/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes to the specs you want to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/inference\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"inference\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"inference_tlt\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['inference_tlt']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "job_id = job_map[\"inference_tlt\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xvf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "inference_out_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference output must be here\n",
    "!ls {inference_out_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print Classification results\n",
    "!cat {inference_out_path}/result.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
