{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Notebook to demonstrate TAO-Remote Client AutoML workflow for Image Classification\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![image](https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
    "\n",
    "\n",
    "### Learning Objective\n",
    "\n",
    "This AutoML notebook applies to identifying the optimal hyperparameters (e.g., learning rate, batch size, weight regularizer, number of layers, etc.) in order to obtain better accuracy results or converge faster on AI models for classification application. \n",
    "- Take a pretrained model and choose automl algorithm/parameters to start AutoML train.\n",
    "- At the end of an AutoML run, you will receive a config file that specifies the best performing model, along with the binary model file to deploy it to your application.\n",
    "\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "- Creating train and eval dataset\n",
    "- Upload datasets to the service\n",
    "- Set AutoML algorithm configurations\n",
    "  - Add/Remove AutoML parameters\n",
    "- Override train config defaults\n",
    "- Run AutoML\n",
    "\n",
    "\n",
    "### AutoML Workflow\n",
    "\n",
    "User starts with selecting model topology, create and upload dataset, configuring parameters, training with AutoML to comparing the model.\n",
    "\n",
    "![image](https://raw.githubusercontent.com/vpraveen-nv/model_card_images/main/api/automl_workflow.png)\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Install TAO remote client](#head-1)\n",
    "1. [Set the remote service base URL](#head-2)\n",
    "1. [Access the shared volume](#head-3)\n",
    "1. [Create the datasets](#head-4)\n",
    "1. [List datasets](#head-5)\n",
    "1. [Create a model experiment](#head-6)\n",
    "1. [Find pretrained model](#head-7)\n",
    "1. [Set AutoML related configurations](#head-8)\n",
    "1. [Provide train specs](#head-9)\n",
    "1. [Run AutoML train](#head-10)\n",
    "1. [Get the best model from AutoML](#head-11)\n",
    "1. [Delete experiment](#head-12)\n",
    "1. [Delete datasets](#head-13)\n",
    "1. [Unmount shared volume](#head-14)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import getpass\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "namespace = 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign a model_name in FIXME 1\n",
    "2. Choose between default or custom dataset in FIXME 2\n",
    "3. Assign the ip_address and port_number in FIXME 3 and FIXME 4 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "3. Set NGC API key in FIXME 5\n",
    "4. Assign path of data directory in FIXME 6\n",
    "5. Choose between Bayesian and Hyperband automl_algorithm in FIXME 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Available models (#FIXME 1):\n",
    "# 1. classification - https://docs.nvidia.com/tao/tao-toolkit/text/image_classification.html\n",
    "# 2. multitask-classification - https://docs.nvidia.com/tao/tao-toolkit/text/multitask_image_classification.html\n",
    "# classification is the same as multi-class classification\n",
    "\n",
    "model_name = \"multitask-classification\"  # FIXME1 (Add the model name from the above mentioned list)\n",
    "dataset_to_be_used = \"default\" # FIXME2 example: default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Install TAO remote client <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO-Client wheel.\n",
    "! pip3 install nvidia-tao-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the version of the TAO-Client\n",
    "! tao-client --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Set the remote service base URL <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the node_addr and port number\n",
    "node_addr = \"<ip_address>\" # FIXME3 example: 10.137.149.22\n",
    "node_port = \"<port_number>\" # FIXME4 example: 32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'\n",
    "%env BASE_URL=http://{node_addr}:{node_port}/{namespace}/api/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $BASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIXME: Set ngc_api_key valiable\n",
    "ngc_api_key = \"<ngc_api_key>\" # FIXME5 example: zZYtczM5amdtdDcwNjk0cnA2bGU2bXQ3bnQ6NmQ4NjNhMDItMTdmZS00Y2QxLWI2ZjktNmE5M2YxZTc0OGyM\n",
    "\n",
    "# Exchange NGC_API_KEY for JWT\n",
    "identity = json.loads(subprocess.getoutput(f'tao-client login --ngc-api-key {ngc_api_key}'))\n",
    "\n",
    "%env USER={identity['user_id']}\n",
    "%env TOKEN={identity['token']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Access the shared volume <a class=\"anchor\" id=\"head-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get PVC ID\n",
    "pvc_id = subprocess.getoutput(f'kubectl get pvc tao-toolkit-api-pvc -n {namespace} -o jsonpath=\"{{.spec.volumeName}}\"')\n",
    "print(pvc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get NFS server info\n",
    "provisioner = json.loads(subprocess.getoutput(f'helm get values nfs-subdir-external-provisioner -o json'))\n",
    "nfs_server = provisioner['nfs']['server']\n",
    "nfs_path = provisioner['nfs']['path']\n",
    "print(nfs_server, nfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user = getpass.getuser()\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "! echo \"Password for {user}\"\n",
    "password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mount shared volume \n",
    "! mkdir -p ~/shared\n",
    "\n",
    "command = \"apt-get -y install nfs-common >> /dev/null\"\n",
    "! echo {password} | sudo -S -k {command}\n",
    "\n",
    "command = f\"mount -t nfs {nfs_server}:{nfs_path}/{namespace}-tao-toolkit-api-pvc-{pvc_id} ~/shared\"\n",
    "! echo {password} | sudo -S -k {command} && echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Create the datasets <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "**For multi-class classification:**\n",
    "\n",
    "We will be using the `pascal VOC dataset` for the tutorial. To find more details please visit [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit). Please download the dataset present [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar) to the environment variable $DATA_DIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure, and skip running** \"**Split dataset into train and val sets**\"\n",
    "```\n",
    "DATA_DIR\n",
    "├── images_test\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_1.jpg\n",
    "│   │   ├── image_name_2.jpg\n",
    "│   │   ├── ...\n",
    "|   |   ... \n",
    "│   └── class_name_n\n",
    "│       ├── image_name_3.jpg\n",
    "│       ├── image_name_4.jpg\n",
    "│       ├── ...\n",
    "├── images_train\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_5.jpg\n",
    "│   │   ├── image_name_6.jpg\n",
    "|   |   ...\n",
    "│   └── class_name_n\n",
    "│       ├── image_name_7.jpg\n",
    "│       ├── image_name_8.jpg\n",
    "│       ├── ...\n",
    "|\n",
    "└── images_val\n",
    "    ├── class_name_1\n",
    "    │   ├── image_name_9.jpg\n",
    "    │   ├── image_name_10.jpg\n",
    "    │   ├── ...\n",
    "    |   ...\n",
    "    └── class_name_n\n",
    "        ├── image_name_11.jpg\n",
    "        ├── image_name_12.jpg\n",
    "        ├── ...\n",
    "```\n",
    "- Each class name folder should contain the images corresponding to that class\n",
    "- Same class name folders should be present across images_test, images_train and images_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For multi-task classification:**\n",
    "\n",
    "We will be using the Fashion Product Images (Small) for the tutorial. This dataset is available on Kaggle.In this tutorial, our trained classification network will perform three tasks: article category classification, base color classification and target season classification.\n",
    "\n",
    "To download the dataset, you will need a Kaggle account. After login, you can download the dataset zip file [here](https://www.kaggle.com/paramaggarwal/fashion-product-images-small). The downloaded file is archive.zip with a subfolder called myntradataset. Unzip contents in this subfolder to your workdir created in the cell above and you should have a folder called images and a CSV file called styles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure**\n",
    "```\n",
    "DATA_DIR\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   |   ├── ...\n",
    "├── styles.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = model_name # FIXME6\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_to_be_used == \"default\":\n",
    "    if model_name == \"classification\":\n",
    "        if not os.path.exists(os.path.join(DATA_DIR,\"VOCtrainval_11-May-2012.tar\")):\n",
    "            print(\"Download VOC tar data into \", DATA_DIR)\n",
    "        else:\n",
    "            !tar -xf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR\n",
    "    elif model_name == \"multitask-classification\":\n",
    "        if not os.path.exists(os.path.join(DATA_DIR,\"archive.zip\")):\n",
    "            print(f\"Download Fashion zip data into \", DATA_DIR)\n",
    "        else:\n",
    "            !unzip -uq $DATA_DIR/archive.zip -d $DATA_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset is present\n",
    "if model_name == \"classification\" and dataset_to_be_used == \"default\":\n",
    "    !if [ ! -d $DATA_DIR/VOCdevkit ]; then echo 'Images folder NOT found.'; else echo 'Found images folder.';fi\n",
    "    !rm -rf $DATA_DIR/split\n",
    "elif model_name == \"multitask-classification\":\n",
    "    !if [ ! -d $DATA_DIR/images ]; then echo 'Images folder NOT found.'; else echo 'Found images folder.';fi\n",
    "    !if [ ! -f $DATA_DIR/styles.csv ]; then echo 'CSV file NOT found.'; else echo 'Found CSV file.';fi\n",
    "    # Create subdirectories and remove existing files in them\n",
    "    !mkdir -p $DATA_DIR/images_train && rm -rf $DATA_DIR/images_train/*\n",
    "    !mkdir -p $DATA_DIR/images_val && rm -rf $DATA_DIR/images_val/*\n",
    "    !mkdir -p $DATA_DIR/images_test && rm -rf $DATA_DIR/images_test/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and val sets\n",
    "if model_name == \"classification\" and dataset_to_be_used == \"default\":\n",
    "    !python3 -m pip install tqdm\n",
    "    from os.path import join as join_path\n",
    "    import os\n",
    "    import glob\n",
    "    import re\n",
    "    import shutil\n",
    "\n",
    "    DATA_DIR=os.environ.get('DATA_DIR')\n",
    "    source_dir = join_path(DATA_DIR, \"VOCdevkit/VOC2012\")\n",
    "    target_dir = join_path(DATA_DIR, \"formatted\")\n",
    "\n",
    "\n",
    "    suffix = '_trainval.txt'\n",
    "    classes_dir = join_path(source_dir, \"ImageSets\", \"Main\")\n",
    "    images_dir = join_path(source_dir, \"JPEGImages\")\n",
    "    classes_files = glob.glob(classes_dir+\"/*\"+suffix)\n",
    "    for file in classes_files:\n",
    "        # get the filename and make output class folder\n",
    "        classname = os.path.basename(file)\n",
    "        if classname.endswith(suffix):\n",
    "            classname = classname[:-len(suffix)]\n",
    "            target_dir_path = join_path(target_dir, classname)\n",
    "            if not os.path.exists(target_dir_path):\n",
    "                os.makedirs(target_dir_path)\n",
    "        else:\n",
    "            continue\n",
    "        print(classname)\n",
    "\n",
    "        with open(file) as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            tokens = re.split('\\s+', line)\n",
    "            if tokens[1] == '1':\n",
    "                # copy this image into target dir_path\n",
    "                target_file_path = join_path(target_dir_path, tokens[0] + '.jpg')\n",
    "                src_file_path = join_path(images_dir, tokens[0] + '.jpg')\n",
    "                shutil.copyfile(src_file_path, target_file_path)\n",
    "    \n",
    "    from random import shuffle\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    DATA_DIR=os.environ.get('DATA_DIR')\n",
    "    SOURCE_DIR=os.path.join(DATA_DIR, 'formatted')\n",
    "    TARGET_DIR=os.path.join(DATA_DIR,'split')\n",
    "    # list dir\n",
    "    print(os.walk(SOURCE_DIR))\n",
    "    dir_list = next(os.walk(SOURCE_DIR))[1]\n",
    "    # for each dir, create a new dir in split\n",
    "    for dir_i in tqdm(dir_list):\n",
    "        newdir_train = os.path.join(TARGET_DIR, 'images_train', dir_i)\n",
    "        newdir_val = os.path.join(TARGET_DIR, 'images_val', dir_i)\n",
    "        newdir_test = os.path.join(TARGET_DIR, 'images_test', dir_i)\n",
    "\n",
    "        if not os.path.exists(newdir_train):\n",
    "                os.makedirs(newdir_train)\n",
    "        if not os.path.exists(newdir_val):\n",
    "                os.makedirs(newdir_val)\n",
    "        if not os.path.exists(newdir_test):\n",
    "                os.makedirs(newdir_test)\n",
    "\n",
    "        img_list = glob.glob(os.path.join(SOURCE_DIR, dir_i, '*.jpg'))\n",
    "        # shuffle data\n",
    "        shuffle(img_list)\n",
    "\n",
    "        for j in range(int(len(img_list)*0.7)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'images_train', dir_i))\n",
    "\n",
    "        for j in range(int(len(img_list)*0.7), int(len(img_list)*0.8)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'images_val', dir_i))\n",
    "\n",
    "        for j in range(int(len(img_list)*0.8), len(img_list)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'images_test', dir_i))\n",
    "\n",
    "    print('Done splitting dataset.')\n",
    "\n",
    "elif model_name == \"multitask-classification\" and dataset_to_be_used == \"default\":\n",
    "    !python3 -m pip install numpy\n",
    "    !python3 -m pip install pandas\n",
    "    import os\n",
    "    import shutil\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(os.environ['DATA_DIR'] + '/styles.csv', error_bad_lines=False, warn_bad_lines=False)\n",
    "    df = df[['id', 'baseColour', 'subCategory', 'season']]\n",
    "    df = df.dropna()\n",
    "    category_cls = df.subCategory.value_counts()[:10].index # 10-class multitask-classification\n",
    "    season_cls = ['Spring', 'Summer', 'Fall', 'Winter'] # 4-class multitask-classification\n",
    "    color_cls = df.baseColour.value_counts()[:11].index # 11-class multitask-classification\n",
    "\n",
    "    # Get all valid rows\n",
    "    df = df[df.subCategory.isin(category_cls) & df.season.isin(season_cls) & df.baseColour.isin(color_cls)]\n",
    "    df.columns = ['fname', 'base_color', 'category', 'season']\n",
    "    df.fname = df.fname.astype(str)\n",
    "    df.fname = df.fname + '.jpg'\n",
    "\n",
    "    # remove entries whose image file is missing\n",
    "    all_img_files = os.listdir(os.environ['DATA_DIR'] + '/images')\n",
    "    df = df[df.fname.isin(all_img_files)]\n",
    "\n",
    "    idx = np.arange(len(df))\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_split_idx = int(len(df)*0.8)\n",
    "    train_df = df.iloc[idx[:train_split_idx]]\n",
    "    val_df = df.iloc[idx[train_split_idx:]]\n",
    "\n",
    "    # Add a simple sanity check\n",
    "    assert len(train_df.season.unique()) == 4 and len(train_df.base_color.unique()) == 11 and \\\n",
    "        len(train_df.category.unique()) == 10, 'Training set misses some classes, re-run this cell!'\n",
    "    assert len(val_df.season.unique()) == 4 and len(val_df.base_color.unique()) == 11 and \\\n",
    "        len(val_df.category.unique()) == 10, 'Validation set misses some classes, re-run this cell!'\n",
    "\n",
    "    for image_name in train_df[\"fname\"]:\n",
    "        source_file_name = os.path.join(DATA_DIR, \"images\", image_name)\n",
    "        destination_file_name = os.path.join(DATA_DIR, \"images_train\", image_name)\n",
    "        shutil.copy(source_file_name, destination_file_name)\n",
    "\n",
    "    for image_name in val_df[\"fname\"]:\n",
    "        source_file_name = os.path.join(DATA_DIR, \"images\", image_name)\n",
    "        destination_file_name = os.path.join(DATA_DIR, \"images_train\", image_name)\n",
    "        shutil.copy(source_file_name, destination_file_name)\n",
    "        destination_file_name = os.path.join(DATA_DIR, \"images_val\", image_name)\n",
    "        shutil.copy(source_file_name, destination_file_name)\n",
    "\n",
    "    # save processed data labels\n",
    "    train_df.to_csv(os.environ['DATA_DIR'] + '/train.csv', index=False)\n",
    "    val_df.to_csv(os.environ['DATA_DIR'] + '/val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "if model_name == \"classification\":\n",
    "    !if [ ! -d $DATA_DIR/split/images_train ]; then echo 'train folder NOT found.'; else echo 'Found train images folder.';fi\n",
    "    !if [ ! -d $DATA_DIR/split/images_val ]; then echo 'val folder NOT found.'; else echo 'Found val images folder.';fi\n",
    "    !if [ ! -d $DATA_DIR/split/images_test ]; then echo 'test folder NOT found.'; else echo 'Found test images folder.';fi\n",
    "elif model_name == \"multitask_classification\":\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"Number of images in the train set. {}\".format(\n",
    "        len(pd.read_csv(os.environ['DATA_DIR'] + '/train.csv'))\n",
    "    ))\n",
    "    print(\"Number of images in the validation set. {}\".format(\n",
    "        len(pd.read_csv(os.environ['DATA_DIR'] + '/val.csv'))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and upload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name == \"classification\":\n",
    "    ds_format = \"default\"\n",
    "elif model_name == \"multitask-classification\":\n",
    "    ds_format = \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type image_classification --dataset_format {ds_format}\")\n",
    "print(train_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name == \"classification\":\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/split/images_train ~/shared/users/{os.environ['USER']}/datasets/{train_dataset_id}/\n",
    "elif model_name == \"multitask-classification\":\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/images_train ~/shared/users/{os.environ['USER']}/datasets/{train_dataset_id}/\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/train.csv ~/shared/users/{os.environ['USER']}/datasets/{train_dataset_id}/\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/val.csv ~/shared/users/{os.environ['USER']}/datasets/{train_dataset_id}/\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type image_classification --dataset_format {ds_format}\")\n",
    "print(eval_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name == \"classification\":\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/split/images_val ~/shared/users/{os.environ['USER']}/datasets/{eval_dataset_id}/\n",
    "elif model_name == \"multitask-classification\":\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/images_val ~/shared/users/{os.environ['USER']}/datasets/{eval_dataset_id}\n",
    "    ! rsync -ah --info=progress2 {DATA_DIR}/val.csv ~/shared/users/{os.environ['USER']}/datasets/{eval_dataset_id}/\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### List datasets <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = os.path.join(home, 'shared', 'users', os.environ['USER'], 'datasets', '*', 'metadata.json')\n",
    "\n",
    "datasets = []\n",
    "for metadata_path in glob.glob(pattern):\n",
    "    with open(metadata_path, 'r') as metadata_file:\n",
    "        datasets.append(json.load(metadata_file))\n",
    "\n",
    "print(json.dumps(datasets, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Create a model experiment <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_arch = model_name.replace(\"-\",\"_\")\n",
    "if network_arch == \"classification\":\n",
    "    encode_key = \"nvidia_tlt\"\n",
    "else:\n",
    "    encode_key = \"tlt_encode\"\n",
    "model_id = subprocess.getoutput(f\"tao-client {model_name} model-create --network_arch {network_arch} --encryption_key {encode_key} \")\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign train, eval datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'metadata.json')\n",
    "\n",
    "with open(metadata_path , \"r\") as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "metadata[\"train_datasets\"] = [train_dataset_id]\n",
    "metadata[\"eval_dataset\"] = eval_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Find pretrained model <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning pretrained models to different yolo versions\n",
    "# print base_url+\"/model\" to get the details of all pretrained models and make the appropriate changes to this map for experiments like for example \n",
    "# you are changing the number of layers to 34, then you have to make the appropriate change in the pretrained model name\n",
    "# print(base_url+\"/model\")\n",
    "pretrained_map = {\"classification\" : \"pretrained_classification:resnet18\",\n",
    "                  \"multitask_classification\" : \"pretrained_classification:resnet10\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = os.path.join(home, 'shared', 'users', '*', 'models', '*', 'metadata.json')\n",
    "\n",
    "ptm_id = None\n",
    "for ptm_metadata_path in glob.glob(pattern):\n",
    "  with open(ptm_metadata_path, 'r') as metadata_file:\n",
    "    ptm_metadata = json.load(metadata_file)\n",
    "    ngc_path = ptm_metadata.get(\"ngc_path\")\n",
    "    metadata_network_arch = ptm_metadata.get(\"network_arch\")\n",
    "    if metadata_network_arch == network_arch and ngc_path.endswith(pretrained_map[network_arch]):\n",
    "      ptm_id = ptm_metadata[\"id\"]\n",
    "      break\n",
    "\n",
    "metadata[\"ptm\"] = ptm_id\n",
    "print(ptm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View hyperparameters that are enabled for AutoML by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View default automl specs enabled\n",
    "! tao-client {model_name} model-automl-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/automl_defaults.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Set AutoML related configurations <a class=\"anchor\" id=\"head-8\"></a>\n",
    "Refer to these hyper-links to see the parameters supported by each network and add more parameters if necessary in addition to the default automl enabled parameters: [Multiclass_classification](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_action_specs.html#train), \n",
    "[Multitask_classification](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_action_specs.html#id27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose automl algorithm between \"Bayesian\" and \"HyperBand\".\n",
    "automl_algorithm=\"Bayesian\" # FIXME7 example: Bayesian/HyperBand\n",
    "\n",
    "metric = \"kpi\" #Don't change this, in future multiple metrics will be supported\n",
    "additional_automl_parameters = [] #Refer to parameter list mentioned in the above links and add any extra parameter in addition to the default enabled ones\n",
    "remove_default_automl_parameters = [] #Remove any hyperparameters that are enabled by default for AutoML\n",
    "\n",
    "metadata[\"automl_algorithm\"] = automl_algorithm\n",
    "metadata[\"automl_enabled\"] = True\n",
    "metadata[\"metric\"] = metric\n",
    "metadata[\"automl_add_hyperparameters\"] = str(additional_automl_parameters)\n",
    "metadata[\"automl_remove_hyperparameters\"] = str(remove_default_automl_parameters)\n",
    "\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=2)\n",
    "\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Provide train specs <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default train model specs\n",
    "! tao-client {model_name} model-train-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize train model specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', 'train.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Apply changes for any of the parameters listed in the previous cell as required\n",
    "# Example for multitask-classification (for each network the parameter key might be different)\n",
    "specs[\"training_config\"][\"num_epochs\"] = 10\n",
    "# Example for classification\n",
    "# specs[\"train_config\"][\"n_epochs\"] = 80\n",
    "\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Run AutoML train <a class=\"anchor\" id=\"head-10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_job_id = subprocess.getoutput(f\"tao-client {model_name} model-train --id \" + model_id)\n",
    "print(train_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to print log file for the upcoming cell\n",
    "def my_tail(logs_dir, log_file):\n",
    "    %env LOG_FILE={logs_dir}/{log_file}\n",
    "    ! mkdir -p {logs_dir}\n",
    "    ! [ ! -f \"$LOG_FILE\" ] && touch $LOG_FILE && chmod 666 $LOG_FILE\n",
    "    ! tail -f -n +1 $LOG_FILE | while read LINE; do echo \"$LINE\"; [[ \"$LINE\" == \"EOF\" ]] && pkill -P $$ tail; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set poll_automl_stats to True if just want to see what's the time left, how many epochs are remaining etc.\n",
    "# Set poll_automl_stats to False if you want to skip stats and see the training logs instead. Training logs viewing are supported only for Bayesian\n",
    "\n",
    "# Training times for different models benchmarked on 1 GPU V100 machine can be found here: https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#results-of-automl-experiments\n",
    "\n",
    "poll_automl_stats = True\n",
    "if poll_automl_stats:\n",
    "    import time\n",
    "    from IPython.display import clear_output\n",
    "    stats_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, train_job_id, \"automl_metadata.json\")\n",
    "    controller_json_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, train_job_id, \"controller.json\")\n",
    "    while True:\n",
    "        time.sleep(15)\n",
    "        clear_output(wait=True)\n",
    "        if os.path.exists(stats_path):\n",
    "            try:\n",
    "                with open(stats_path , \"r\") as stats_file:\n",
    "                    stats_dict = json.load(stats_file)\n",
    "                print(json.dumps(stats_dict, indent=2))\n",
    "                if float(stats_dict[\"Number of epochs yet to start\"]) == 0.0:\n",
    "                    break\n",
    "            except (json.JSONDecodeError):\n",
    "                print(\"Stats computed are being written to file. Stats will be visible on screen in a few seconds\")\n",
    "else:\n",
    "    # Print the log file - supported only for bayesian (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "    if automl_algorithm == \"Bayesian\":\n",
    "        logs_dir = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id)\n",
    "        max_recommendations = metadata.get(\"automl_max_recommendations\",20)\n",
    "        for experiment_num in range(max_recommendations):\n",
    "            log_file = f\"{train_job_id}/experiment_{experiment_num}/log.txt\"\n",
    "            while True:\n",
    "                if os.path.exists(os.path.join(logs_dir, log_file)):\n",
    "                    break\n",
    "            print(f\"\\n\\nViewing experiment {experiment_num}\\n\\n\")\n",
    "            my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best model from AutoML <a class=\"anchor\" id=\"head-11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The config and the weights of the best configuration are present at best_model folder\n",
    "# Takes a few seconds to copy the original automl experiment to best_model folder\n",
    "!python3 -m pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "automl_job_dir = f\"{home}/shared/users/{os.environ['USER']}/models/{model_id}/{train_job_id}\"\n",
    "best_model_path =  f\"{automl_job_dir}/best_model\"\n",
    "\n",
    "while True:\n",
    "    if os.path.exists(best_model_path) and len(os.listdir(best_model_path)) > 0 and os.path.exists(f\"{best_model_path}/controller.json\"):\n",
    "        #List the binary model file\n",
    "        print(\"\\nCheckpoints for the best performing experiment\")\n",
    "        if os.path.exists(best_model_path+\"/weights\") and len(os.listdir(best_model_path+\"/weights\")) > 0:\n",
    "            print(f\"Folder: {best_model_path}/weights\")\n",
    "            print(\"Files:\", os.listdir(best_model_path+\"/weights\"))\n",
    "        else:\n",
    "            print(f\"Folder: {best_model_path}\")\n",
    "            print(\"Files:\", os.listdir(best_model_path))\n",
    "\n",
    "        experiment_artifacts = json.load(open(f\"{best_model_path}/controller.json\",\"r\"))\n",
    "        data_frame = pd.DataFrame(experiment_artifacts)\n",
    "        # Print experiment id/number and the corresponding result\n",
    "        print(\"\\nResults of all experiments\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "            print(data_frame[[\"id\",\"result\"]])\n",
    "\n",
    "        print(\"\\nConfig/Spec file for the best performing experiment (recommendation_id.kitti with the maximum result value in the dataframe)\")\n",
    "        # List the recommendation config file of the best performing checkpoint(recommendation_id.kitti with the maximum result value in the dataframe)\n",
    "        !ls {best_model_path}/*.kitti \n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Delete experiment <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/models/{model_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    }
   },
   "source": [
    "### Delete datasets <a class=\"anchor\" id=\"head-13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/datasets/{train_dataset_id}\n",
    "! rm -rf ~/shared/users/{os.environ['USER']}/datasets/{eval_dataset_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unmount shared volume <a class=\"anchor\" id=\"head-14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "command = \"umount ~/shared\"\n",
    "! echo {password} | sudo -S -k {command} && echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uninstall TAO Remote Client <a class=\"anchor\" id=\"head-32\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip3 uninstall -y nvidia-tao-client"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
