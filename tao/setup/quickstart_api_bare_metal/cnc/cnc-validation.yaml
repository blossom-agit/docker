- hosts: master
  gather_facts: yes
  vars_files:
    - cnc_values.yaml
    - gpu_operator.yaml
  tasks:
    - name: Waiting for the Cluster to become available 
      args:
        executable: /bin/bash
      shell: |
        state=$(kubectl get pods -n nvidia-gpu-operator | egrep -v 'Running|Completed|NAME' | wc -l)
        while [ $state != 0 ]
          do
            sleep 10
            state=$(kubectl get pods -n nvidia-gpu-operator | egrep -v 'Running|Completed|NAME' | wc -l)
          done
      register: status
      when: "cnc_version > 4.1 and ansible_architecture == 'x86_64'"

    - name: Validate kubernetes cluster is up
      shell:  kubectl cluster-info | grep master
      register: cluster
      failed_when: false
      ignore_errors: yes

    - name: Check Operating System Version
      shell: cat /etc/os-release | grep -iw version | sed 's/VERSION=//g;s/"//g'
      register: osversion
      ignore_errors: yes

    - name: Check Docker Version
      become: true
      when: cnc_version <= 3.1
      shell: docker version | grep -i -A2 'Server' | grep Version  | awk '{print $2}'
      register: dockerversion
      ignore_errors: yes

    - name: Check Containerd Version
      become: true
      when: cnc_version >= 4.0
      shell: containerd -v | awk '{print $3}'
      register: containerd_version
      ignore_errors: yes

    - name: Check Kubernetes Version
      shell: kubectl get nodes | awk '{print $NF}' | grep -v VERSION
      register: k8sversion
      ignore_errors: yes

    - name: Check Helm Version
      shell: helm version --short | sed 's/v//g;s/\+.*//g'
      register: helmversion
      ignore_errors: yes

    - name: Check Nvidia GPU Operator Toolkit versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep toolkit | awk -F':' '{print $2}' | awk -F'-' '{print $1}' | head -n1
      register: nvtoolkit
      ignore_errors: yes

    - name: Check Nvidia K8s Device versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'k8s-device' | awk -F':' '{print $2}' | head -n1 | sed 's/-ubi8//g'
      register: k8sdevice
      ignore_errors: yes

    - name: Check GPU Operator Nvidia Container Driver versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'nvidia/driver'  | awk -F':' '{print $2}' | awk -F'-' '{print $1}' | head -n1
      register: nvcdriver
      ignore_errors: yes

    - name: Check GPU Operator Nvidia DGCM Versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'k8s/dcgm'  |  awk -F':' '{print $2}'  | sed 's/-ubuntu.*//g'
      register: dgcm
      ignore_errors: yes

    - name: Check GPU Operator Node Feature Discovery Versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'node-feature-discovery' | head -n1 |  awk -F':' '{print $2}'
      register: nodediscover
      ignore_errors: yes

    - name: Check GPU Operator GPU Feature Discovery Versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'nvidia/gpu-feature-discovery'  | awk -F':' '{print $2}' | head -n1 | sed 's/-ubi8//g'
      register: gpudiscover
      ignore_errors: yes

    - name: Check Nvidia GPU Operator versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'nvidia/gpu-operator'  | awk -F':' '{print $2}' | head -n1
      register: gpuoperator
      ignore_errors: yes

    - name: Check Nvidia MIG Maanger versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'mig-manager'  | awk -F':' '{print $2}' | head -n1 | sed 's/-ubuntu20.04//'g
      register: mig_manager
      ignore_errors: yes

    - name: Check Nvidia validator versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep validator | awk '{print $2}' | uniq | awk -F':' '{print $2}' | head -n1
      register: validator
      ignore_errors: yes

    - name: Check Nvidia DCGM Exporter versions
      shell: kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep dcgm-e | awk '{print $2}' | uniq | awk -F':' '{print $2}' | sed 's/-ubuntu20.04//'g
      register: dcgm_exporter
      ignore_errors: yes

    - name: Check NVIDIA Driver Version
      when: cnc_nvidia_driver == true 
      shell: nvidia-smi --query-gpu=driver_version --format=csv,noheader --id=0
      register: nvidia_driver

    - name: Check NVIDIA Container ToolKit Version
      when: cnc_docker == true
      become: true
      shell: dpkg -l | grep -i nvidia-container-toolkit | awk '{ print $3}'
      register: nvidia_ct

    - name: Check Mellanox Network Operator version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n network-operator -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}" | grep 'network-operator'  | awk -F':' '{print $2}' | head -n1
      register: networkoperator
      ignore_errors: yes

    - name: Check Mellanox MOFED Driver Version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n nvidia-network-operator-resources -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}"  | grep mellanox/mofed | awk -F'mofed-' '{print $3}' | awk -F":" '{print $1}' | head -n1
      register: mofed_version
      ignore_errors: yes

    - name: Check RDMA Shared Device Plugin Version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n nvidia-network-operator-resources -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}"  | grep rdma-shared | awk -F':' '{print $2}' | head -n1
      register: rdma_version
      ignore_errors: yes

    - name: Check SRIOV Device Plugin Version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n nvidia-network-operator-resources -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}"  | grep sriov-device | awk -F':' '{print $2}' | head -n1
      register: sriov_version
      ignore_errors: yes

    - name: Check Container Networking Plugins Version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n nvidia-network-operator-resources -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}"  | grep plugins | awk -F':' '{print $2}' | head -n1
      register: cni_version
      ignore_errors: yes

    - name: Check Multus Version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n nvidia-network-operator-resources -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}"  | grep multus | awk -F':' '{print $2}' | head -n1
      register: multus_version
      ignore_errors: yes

    - name: Check Whereabouts Version
      when: "enable_network_operator == true"
      shell: kubectl get pods -n nvidia-network-operator-resources -o=jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].image}{'\n'}"  | grep whereabouts | awk -F':' '{print $2}' | head -n1
      register: whereabouts_version
      ignore_errors: yes

    - name: check master node is up and running
      shell: kubectl get nodes | grep -i ready
      register: nodeready
      failed_when: "'NotReady' in nodeready.stdout"
      ignore_errors: yes

    - name: Check all pods are running for Kubernetes
      shell: kubectl get pods --all-namespaces | egrep -iv 'Running|NAME|Completed'
      register: kubepods
      failed_when: kubepods.rc == 0
      ignore_errors: yes

    - name: validate helm installed
      shell: helm ls
      register: helmls
      failed_when: helmls.rc == 1
      ignore_errors: yes


    - name: Collecting Number of GPU's
      shell: "kubectl describe nodes | grep -A 6 Capacity | grep 'nvidia.com/gpu' | awk '{print $2}' | head -n1 | tr -d '\n'"
      register: gpus
      ignore_errors: yes

    - name: Create NVIDIA-SMI yaml
      when: cnc_version >= 7.0
      copy:
        content:  |
                 apiVersion: v1
                 kind: Pod
                 metadata:
                   name: nvidia-smi
                 spec:
                   restartPolicy: OnFailure
                   containers:
                     - name: nvidia-smi
                       image: "nvidia/cuda:11.7.0-base-ubuntu22.04"
                       args: ["nvidia-smi"]
        dest: nvidia-smi.yaml

    - name: Report Operating System Version of RHEL/CentOS
      when: "ansible_distribution in ['RedHat', 'CentOS']"
      ignore_errors: yes
      debug:
        msg: "RHEL/CentOS Operating System version {{ osversion.stdout }}"

    - name: Report Operating System Version of Ubuntu
      when: "ansible_distribution == 'Ubuntu'"
      ignore_errors: yes
      debug:
        msg: "Ubuntu Operating System version {{ osversion.stdout }}"

    - name: Report Docker Version
      when: cnc_version <= 3.1
      ignore_errors: yes
      debug:
        msg: "Docker Version {{ dockerversion.stdout }}"

    - name: Report Containerd Version
      when: cnc_version >= 4.0
      ignore_errors: yes
      debug:
        msg: " Containerd Version {{ containerd_version.stdout }}"

    - name: Report Kubernetes Version
      ignore_errors: yes
      debug:
        msg: "Kubernetes Version {{ k8sversion.stdout }}"

    - name: Report Helm Version
      ignore_errors: yes
      debug:
        msg: "Helm Version {{ helmversion.stdout }}"

    - name: Report Nvidia GPU Operator version
      ignore_errors: yes
      debug:
        msg: "Nvidia GPU Operator versions {{ gpuoperator.stdout }}"

    - name: Report Nvidia Container Driver Version
      when: cnc_docker == false 
      ignore_errors: yes
      debug:
        msg: "Nvidia Container Driver Version {{ nvcdriver.stdout }}"

    - name: Report GPU Operator NV Toolkit Driver
      when: cnc_docker == false
      ignore_errors: yes
      debug:
        msg: "NV Container Toolkit Version {{ nvtoolkit.stdout }}"

    - name: Report Nvidia Container Driver Version
      when: cnc_docker == true
      ignore_errors: yes
      debug:
        msg: "Nvidia Container Driver Version {{ nvidia_driver.stdout }}"

    - name: Report GPU Operator NV Toolkit Driver
      when: cnc_docker == true
      ignore_errors: yes
      debug:
        msg: "NV Container Toolkit Version {{ nvidia_ct.stdout }}"

    - name: Report K8sDevice Plugin Version
      ignore_errors: yes
      debug:
        msg: "Nvidia K8s Device Plugin Version {{ k8sdevice.stdout }}"

    - name: Report Data Center GPU Manager (DCGM) Version
      ignore_errors: yes
      debug:
        msg: "Data Center GPU Manager (DCGM) Version {{ dgcm.stdout }}"

    - name: Report Node Feature Discovery Version
      ignore_errors: yes
      debug:
        msg: "Node Feature Discovery Version {{ nodediscover.stdout }}"

    - name: Report GPU Feature Discovery Version
      ignore_errors: yes
      debug:
        msg: "GPU Feature Discovery Version {{ gpudiscover.stdout }}"

    - name: Report Nvidia validator version
      ignore_errors: yes
      debug:
        msg: "Nvidia validator version {{ validator.stdout }}"

    - name: Report Nvidia DCGM Exporter version
      ignore_errors: yes
      debug:
        msg: "Nvidia DCGM Exporter version {{ dcgm_exporter.stdout }}"

    - name: Report Nvidia MIG Maanger version
      ignore_errors: yes
      debug:
        msg: "Nvidia MIG Maanger version {{ mig_manager.stdout }}"

    - name: Componenets Matrix Versions Vs Installed Versions
      when: cnc_version == 6.1
      register: compare_61
      args:
        executable: /bin/bash
      shell: |
        echo -e "==========================================================================================="
        echo -e "  Components                          Matrix Version          ||     Installed Version    "
        echo -e "==========================================================================================="
        echo -e "GPU Operator Version                  {{ release_1_10['gpu_operator_version'] }}                 ||     {{ gpuoperator.stdout }}"
        echo -e "Nvidia Container Driver Version       {{ release_1_10['gpu_driver_version'] }}               ||     {{ nvcdriver.stdout }}"
        echo -e "GPU Operator NV Toolkit Driver        {{ release_1_10['container_toolkit'] }}                  ||     {{ nvtoolkit.stdout }}"
        echo -e "K8sDevice Plugin Version              {{ release_1_10['device_plugin']  }}                 ||     {{ k8sdevice.stdout }}"
        echo -e "Data Center GPU Manager(DCGM) Version {{ release_1_10['dcgm_exporter_version'] }}             ||     {{ dcgm_exporter.stdout }}"
        echo -e "Node Feature Discovery Version        {{ release_1_10['nfd_version'] }}                 ||     {{ nodediscover.stdout }}"
        echo -e "GPU Feature Discovery Version         {{ release_1_10['gfd_version'] }}                  ||     {{ gpudiscover.stdout }}"
        echo -e "Nvidia validator version              {{ release_1_10['validator_version'] }}                 ||     {{ validator.stdout }}"
        echo -e "Nvidia MIG Manager version            {{ release_1_10['mig_manager_version'] }}                   ||     {{ mig_manager.stdout }}"
        echo -e
        echo -e "Note: NVIDIA Mig Manager is valid for only Amphere GPU's like A100, A30"
        echo -e
        echo -e "Please validate between Matrix Version and Installed Version listed above"

    - name: Report Versions
      when: cnc_version == 6.1
      debug:
        msg: "{{ compare_61.stdout_lines }}"

    - name: Componenets Matrix Versions Vs Installed Versions
      when: cnc_docker == false and cnc_version == 6.2 or cnc_docker == false and cnc_version == 7.0
      register: compare_70
      args:
        executable: /bin/bash
      shell: |
        echo -e "==========================================================================================="
        echo -e "  Components                          Matrix Version          ||     Installed Version    "
        echo -e "==========================================================================================="
        echo -e "GPU Operator Version                  {{ release_1_11_0['gpu_operator_version'] }}                 ||     {{ gpuoperator.stdout }}"
        echo -e "Nvidia Container Driver Version       {{ release_1_11_0['gpu_driver_version'] }}               ||     {{ nvcdriver.stdout }}"
        echo -e "GPU Operator NV Toolkit Driver        {{ release_1_11_0['container_toolkit'] }}                 ||     {{ nvtoolkit.stdout }}"
        echo -e "K8sDevice Plugin Version              {{ release_1_11_0['device_plugin']  }}                 ||     {{ k8sdevice.stdout }}"
        echo -e "Data Center GPU Manager(DCGM) Version {{ release_1_11_0['dcgm_exporter_version'] }}             ||     {{ dcgm_exporter.stdout }}"
        echo -e "Node Feature Discovery Version        {{ release_1_11_0['nfd_version'] }}                 ||     {{ nodediscover.stdout }}"
        echo -e "GPU Feature Discovery Version         {{ release_1_11_0['gfd_version'] }}                  ||     {{ gpudiscover.stdout }}"
        echo -e "Nvidia validator version              {{ release_1_11_0['validator_version'] }}                 ||     {{ validator.stdout }}"
        echo -e "Nvidia MIG Manager version            {{ release_1_11_0['mig_manager_version'] }}                   ||     {{ mig_manager.stdout }}"
        echo -e
        echo -e "NOTE: NVIDIA Mig Manager is valid for only Amphere GPU's like A100, A30"
        echo -e
        echo -e "Please validate between Matrix Version and Installed Version listed above"

    - name: Report Versions
      when: cnc_docker == false and cnc_version == 6.2 or cnc_docker == false and cnc_version == 7.0
      debug:
        msg: "{{ compare_70.stdout_lines }}"

    - name: Componenets Matrix Versions Vs Installed Versions
      when: cnc_docker == true and cnc_version == 6.2 or cnc_docker == true and cnc_version == 7.0
      register: compare_70
      args:
        executable: /bin/bash
      shell: |
        echo -e "==========================================================================================="
        echo -e "  Components                          Matrix Version          ||     Installed Version    "
        echo -e "==========================================================================================="
        echo -e "GPU Operator Version                  {{ release_1_11_0['gpu_operator_version'] }}                 ||     {{ gpuoperator.stdout }}"
        echo -e "Nvidia Container Driver Version       {{ release_1_11_0['gpu_driver_version'] }}               ||     {{ nvidia_driver.stdout }}"
        echo -e "NVIDIA Toolkit Driver                 {{ release_1_11_0['container_toolkit'] }}                 ||     {{ nvidia_ct.stdout }}"
        echo -e "K8sDevice Plugin Version              {{ release_1_11_0['device_plugin']  }}                 ||     {{ k8sdevice.stdout }}"
        echo -e "Data Center GPU Manager(DCGM) Version {{ release_1_11_0['dcgm_exporter_version'] }}             ||     {{ dcgm_exporter.stdout }}"
        echo -e "Node Feature Discovery Version        {{ release_1_11_0['nfd_version'] }}                 ||     {{ nodediscover.stdout }}"
        echo -e "GPU Feature Discovery Version         {{ release_1_11_0['gfd_version'] }}                  ||     {{ gpudiscover.stdout }}"
        echo -e "Nvidia validator version              {{ release_1_11_0['validator_version'] }}                 ||     {{ validator.stdout }}"
        echo -e "Nvidia MIG Manager version            {{ release_1_11_0['mig_manager_version'] }}                   ||     {{ mig_manager.stdout }}"
        echo -e
        echo -e "NOTE: NVIDIA Mig Manager is valid for only Amphere GPU's like A100, A30"
        echo -e
        echo -e "Please validate between Matrix Version and Installed Version listed above"

    - name: Report Versions
      when: cnc_docker == true and cnc_version == 6.2 or cnc_docker == true and cnc_version == 7.0
      debug:
        msg: "{{ compare_70.stdout_lines }}"

    - name: Validate the GPU Operator pods State
      shell: kubectl get pods --all-namespaces | egrep -v 'kube-system|NAME'
      register: pods
      failed_when: pods.rc == 1
      ignore_errors: yes

    - name: Report GPU Operator Pods
      debug:
        msg: "{{ pods.stdout_lines }}"

    - name: Validate GPU Operator Version for Cloud Native Core 6.2 and 7.0
      shell: helm ls -A | grep gpu-operator |  awk '{print $NF}' | grep -v VERSION | sed 's/v//g' | tr -d '\n\r'
      register: operator_version
      failed_when: cnc_version >= 6.2 and operator_version.stdout != '1.11.0'

    - name: Validate GPU Operator Version for Cloud Native Core 6.1
      shell: helm ls -A | grep gpu-operator |  awk '{print $NF}' | grep -v VERSION | sed 's/v//g'
      register: operator_version
      failed_when: cnc_version == 6.1 and operator_version.stdout != '1.10.1'

    - name: Validating the nvidia-smi on NVIDIA Cloud Native Core
      command: "kubectl run gpu-test --rm -t -i --restart=Never --image=nvidia/cuda:11.4.0-base-ubuntu20.04 -- nvidia-smi"
      when: "cnc_version == 1.3 or cnc_version == 3.1"
      register: smi
      ignore_errors: yes
      async: 60

    - name: Validating the nvidia-smi on NVIDIA Cloud Native Core
      command: "kubectl run gpu-test --rm -t -i --restart=Never --image=nvidia/cuda:11.4.0-base-ubuntu20.04 --limits=nvidia.com/gpu={{ gpus.stdout }} -- nvidia-smi"
      when: "cnc_version >= 4.0 and cnc_version <= 5.2"
      register: smi
      ignore_errors: yes
      async: 60

    - name: Validating the nvidia-smi on NVIDIA Cloud Native Core
      command: "kubectl run gpu-test --rm -t -i --restart=Never --image=nvidia/cuda:11.6.0-base-ubuntu20.04 --limits=nvidia.com/gpu={{ gpus.stdout }} -- nvidia-smi"
      when: "cnc_version == 6.2 or cnc_version == 6.0 or cnc_version == 6.1"
      register: smi
      ignore_errors: yes
      async: 60

    - name: Report Nvidia SMI Validation
      when: cnc_version <= 6.2
      ignore_errors: yes
      debug:
        msg: "{{ smi.stdout_lines }}"

    - name: Validating the CUDA with GPU
      shell: kubectl run cuda-vector-add --rm -t -i --restart=Never --image=k8s.gcr.io/cuda-vector-add:v0.1
      register: cuda
      ignore_errors: yes
      async: 60

    - name: Validating the nvidia-smi on NVIDIA Cloud Native Core 7.0
      shell: "kubectl delete -f nvidia-smi.yaml; sleep 10; kubectl apply -f nvidia-smi.yaml; sleep 25; kubectl logs nvidia-smi"
      when: cnc_version == 7.0
      register: smi
      ignore_errors: yes
      async: 60

    - name: Report Nvidia SMI Validation
      when: cnc_version == 7.0
      ignore_errors: yes
      debug:
        msg: "{{ smi.stdout_lines }}"

    - name: Report Cuda Validation
      ignore_errors: yes
      debug:
        msg: "{{ cuda.stdout_lines }}"

    - name: Report Network Operator version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "Network Operator version {{ networkoperator.stdout }}"

    - name: Report Mellanox MOFED Driver Version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "Mellanox MOFED Driver version {{ mofed_version.stdout }}"

    - name: Report RDMA Shared Device Plugin Version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "RDMA Shared Device Plugin version {{ rdma_version.stdout }}"

    - name: Report SRIOV Device Plugin Version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "SRIOV Device Plugin version {{ sriov_version.stdout }}"

    - name: Report Container Networking Plugin Version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "Container Networking Plugin version {{ cni_version.stdout }}"

    - name: Report Multus Version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "Multus version {{ multus_version.stdout }}"

    - name: Report Whereabouts Version
      when: "enable_network_operator == true"
      ignore_errors: yes
      debug:
        msg: "Container Whereabouts version {{ whereabouts_version.stdout }}"

    - name: Status Check
      shell: echo "All tasks should be changed or ok, if it's failed or ignoring means that validation task failed."
      register: check

    - debug:
        msg: "{{ check.stdout }}"